---
title: "주성분 분석(PCA)"
slug: "주성분 분석(PCA)"
categories: 수학
tags: [선형대수학, 수리통계학, 데이터 분석]
toc: true
redirect_from:
  - /pca
---

데이터 차원 축소 방법 중 하나인 **주성분 분석(Principal Component Analysis, PCA)**을 선형대수학의 관점으로 이해하는 것이 이 글의 목표다.
{: .notice--primary} 

# 데이터 행렬

먼저 데이터를 수학의 언어로 어떻게 다루는가를 정리해보자.

데이터, 또는 데이터 프레임은 행과 열을 갖는 표로 정리된다.

행은 Sample(표본, 케이스) $i$를 ($i=1,\cdots,m)$, 
<br>열은 Feature(변수, 특징, 속성, ...) $j$를 나타낸다. ($j=1,\cdots,n$)

예를 들어 $m$개의 Sample의 2가지 Feature인 키($x_1$)와 몸무게($x_2$)의 데이터를 측정했다고 하자. 
<br>측정 결과를 다음과 같이 $m\times 2$ 표로 정리할 수 있다.

| Sample | 키($x_1$) | 몸무게($x_2$) |
| --- | --- | --- |
| $1$ | Sample $1$의 키 | Sample $1$의 몸무게 |
| $\vdots$ | $\vdots$ | $\vdots$ |
| $m$ | Sample $m$의 키 | Sample $m$의 몸무게 |

이 예시에 따르면 데이터는 $m\times 2$ 행렬($X_0$)이 된다. 

행벡터($x^{(i)T},i=1,\cdots,m$)는 하나의 Sample의 전치를, 
<br>열벡터($x_j,j=1,2$)는 하나의 Feature를 나타낸다.

데이터 행렬 $X_0$는 다음과 같이 3가지 표현이 있으며 서로 같다. 
<br>(순서대로, 전체적인 관점, 열관점, 행관점)

$$
X_0
:=
\begin{bmatrix}
x_1^{(1)}&x_2^{(1)}
\\
\vdots&\vdots
\\
x_1^{(m)}&x_2^{(m)}
\end{bmatrix}
=
\begin{bmatrix}
|&|
\\
x_1&x_2
\\
|&|
\end{bmatrix}
=
\begin{bmatrix}
-x^{(1)T}-
\\
\vdots
\\
-x^{(m)T}-
\end{bmatrix}
$$


note:

(1) $x^{(i)T}$가 $i$번째 Sample이 아니라 $i$번째 Sample의 ‘전치’를 나타낸다.

$i$번째 Sample은 열벡터 $x^{(i)}$이고, $i$번째 Sample의 ‘전치’는 행벡터 $x^{(i)T}$이다. 
<br>
**기본이 열벡터고, 데이터 프레임의 형태로 들어갈 때 전치를 해서 행벡터의 모습으로 들어가는 것**이다. 항상 기본은 열벡터고, 행벡터는 그것의 전치를 해주는 것을 잊지 말자.

(2) 데이터의 값(행렬의 성분)을 $x_{ij}$가 아니라 $x_{j}^{(i)}$라고 하는 것을 주의해야 한다. 

두번째 표기법이 더 편하다는 사실은 데이터 셋을 열 별로 쓸 때 명확히 드러난다. 1번째 열을 먼저 적을 때, $x_1$들을 쭉 적고 그 위에 몇번째 샘플인지만 $(i)$로 표기해주면 된다.

# 평균 벡터, 공분산 행렬

## 회고: 확률변수의 평균, 분산, 공분산

note: 통계학 관련 글이 아니므로 자세한 설명은 생략한다. 
    
변수 $x,y$가 각각 $n$개의 값 $x_1,\cdots,x_n,y_1,\cdots,y_n$을 가질 때,
    
(1) 평균(Mean): 변수의 중심을 나타냄. (어디에 값이 모여있나?)
    
$$
\bar x:=\frac{1}{n}\sum_{i=1}^nx_i
$$
    
(2) 분산(Variance): 변수의 변동성을 나타냄. (얼마만큼 값이 퍼져있나?)
    
$$
s_x^2:=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar x)^2
$$
    
(3) 공분산(Covariance): 두 변수의 동시적인 변동성을 나타냄. (값이 같이 퍼져있는 정도.)
    
$$
s_{xy}^2:=\frac{1}{n-1}\sum_{j=1}^n(x_j-\bar x)(y_j-\bar y)
$$
    
## 재정의: 평균 벡터, 공분산 행렬
변수의 평균, 분산, 공분산의 정의를 데이터 행렬 표기법에 맞는 것으로 다시 써야 한다.

**열은 변수, 행은 샘플!**이라는 사실을 기억할 필요가 있다.

(1) 평균: 각 열에 대한 평균은 $\bar x_1,\bar x_2$가 있다. 이것을 $\bar x_j,j=1,2$라고 표현한다.

$$
\bar x_j:=\frac{1}{m}\sum_{i=1}^m x_j^{(i)},j=1,2
$$

(2) 분산: 각 열에 대한 분산은 $s_1^2,s_2^2$가 있다. 이것을 $s_j^2,j=1,2$라고 표현한다.

$$
s_j^2:=\frac{1}{m-1}\sum_{i=1}^m (x_j^{(i)}-\bar x_j)^2,j=1,2
$$

(3) 공분산: $j,k$열의 공분산은 $s_{jk}^2$이다. 총 4가지가 있다.

$$
s_{jk}^2:=\frac{1}{m-1}\sum_{i=1}^m (x_j^{(i)}-\bar x_j)(x_k^{(i)}-\bar x_k),\\(j,k)=(1,1),(1,2),(2,1),(2,1)
$$

평균을 순서대로 나열한 열벡터를 **평균 벡터**라고 하며, $$\bar x:=\begin{bmatrix}\bar x_1\\\bar x_2\end{bmatrix}$$로 정의한다.

공분산을 모은 행렬은 $$S:=\begin{bmatrix}s_{11}^2&s_{12}^2\\s_{21}^2&s_{22}^2\end{bmatrix}$$이고, **공분산 행렬**이라고 한다.

note:
공분산의 정의에 의해, 같은 열의 공분산은 그 열의 분산이다. 예를 들어 $s_{11}^2=s_1^2$이다. 
<br>그리고 $j,k$열의 공분산은 $k,j$열의 공분산이다. $s_{12}^2=s^2_{21}$. 
<br>따라서 공분산 행렬을 분산-공분산 행렬이라고도 하며 대각성분이 각 열의 분산인 대칭행렬이다.

# 데이터 전처리

PCA를 수행하기 전에 먼저 원 데이터 행렬 $X_0$에 Centering(중심을 0으로 만들기)과 Scaling(크기 조정) 작업을 순서대로 한다. 

Centering은 모든 열의 평균(중심)을 0으로 만드는 작업이다. 

> note: (통계학 토막지식) 특정 변수의 모든 값에서 그 변수의 평균을 빼주면 변수의 평균이 0이 된다.
> 

Scaling은 크기 조절인데, 여기서는 모든 데이터를 $\sqrt{m-1}$로 나누는 작업을 말한다.

이와 같은 Preprocessing(전처리) 작업이 끝난 데이터 행렬(Centered and Scaled)은 $X$라고 하자.

$$
X
:=
\frac{1}{\sqrt{m-1}}
\begin{bmatrix}
x_1^{(1)}-\bar x_1&x_2^{(1)}-\bar x_2
\\
\vdots&\vdots
\\
x_1^{(m)}-\bar x_1&x_2^{(m)}-\bar x_2
\end{bmatrix}
=
\frac{1}{\sqrt{m-1}}
\begin{bmatrix}
-(x^{(1)}-\bar x)^T-
\\
\vdots
\\
-(x^{(m)}-\bar x)^T-
\end{bmatrix}
$$

**이런 작업을 거치면, $X^TX$는 공분산 행렬 $S$가 된다.**

# 공분산 행렬의 계산

이제 공분산 행렬 $S=X^TX$를 직접 계산해보자.

💡 확인: 전체 데이터 관점에서 계산. 확실히 $S=X^TX$가 성립한다.

$$
\begin{split}
X^TX&=\frac{1}{m-1}
\begin{bmatrix}
x_1^{(1)}-\bar x_1&\cdots&x_1^{(m)}-\bar x_1
\\
x_2^{(1)}-\bar x_2&\cdots&x_2^{(m)}-\bar x_2
\end{bmatrix}
\begin{bmatrix}
x_1^{(1)}-\bar x_1&x_2^{(1)}-\bar x_2
\\
\vdots&\vdots
\\
x_1^{(m)}-\bar x_1&x_2^{(m)}-\bar x_2
\end{bmatrix}
\\&=
\frac{1}{m-1}
\begin{bmatrix}
\sum_{i=1}^m(x_1^{(i)}-\bar x_1)^2&\sum_{i=1}^m(x_1^{(i)}-\bar x_1)(x_2^{(i)}-\bar x_2)
\\
\sum_{i=1}^m(x_2^{(i)}-\bar x_2)(x_1^{(i)}-\bar x_1)&\sum_{i=1}^m(x_2^{(i)}-\bar x_2)^2
\end{bmatrix}
\\&=
\begin{bmatrix}
s_{11}^2&s_{12}^2
\\
s_{21}^2&s_{22}^2
\end{bmatrix}
=S
\end{split}
$$

💡 관찰: 행 관점에서 계산한 형태는 어떨까?

$$
\begin{split}
X^TX&=
\frac{1}{m-1}
\begin{bmatrix}
|&&|
\\
(x^{(1)}-\bar x)&\cdots&(x^{(m)}-\bar x)
\\
|&&|
\end{bmatrix}
\begin{bmatrix}
-(x^{(1)}-\bar x)^T-
\\
\vdots
\\
-(x^{(m)}-\bar x)^T-
\end{bmatrix}
\\&=\frac{1}{m-1}\sum_{i=1}^m(x^{(i)}-\bar x)(x^{(i)}-\bar x)^T
\end{split}
$$


💡 결론: 공분산 행렬을 다양한 형태로 표현할 수 있다.

$$
S=\begin{bmatrix}
s_{11}^2&s_{12}^2
\\
s_{21}^2&s_{22}^2
\end{bmatrix}=\frac{1}{m-1}\sum_{i=1}^m(x^{(i)}-\bar x)(x^{(i)}-\bar x)^T=X^TX
$$

# - 중간 요약 -
(0) 원 데이터 $X_0$은 $m$개의 샘플의 2개의 변수에 관한 것이고, $m\times2$ 행렬로 표현된다.

$$
X_0
=
\begin{bmatrix}
x_1^{(1)}&x_2^{(1)}
\\
\vdots&\vdots
\\
x_1^{(m)}&x_2^{(m)}
\end{bmatrix}
=
\begin{bmatrix}
|&|
\\
x_1&x_2
\\
|&|
\end{bmatrix}
=
\begin{bmatrix}
-x^{(1)T}-
\\
\vdots
\\
-x^{(m)T}-
\end{bmatrix}
$$

(1) $X_0$에 전처리를 하여 Centered and Scaled 데이터 $X$를 얻는다.
<br>
(전체 데이터 관점, 행 관점)

$$
X
=
\frac{1}{\sqrt{m-1}}
\begin{bmatrix}
x_1^{(1)}-\bar x_1&x_2^{(1)}-\bar x_2
\\
\vdots&\vdots
\\
x_1^{(m)}-\bar x_1&x_2^{(m)}-\bar x_2
\end{bmatrix}
=
\frac{1}{\sqrt{m-1}}
\begin{bmatrix}
-(x^{(1)}-\bar x)^T-
\\
\vdots
\\
-(x^{(m)}-\bar x)^T-
\end{bmatrix}
$$

(2) 전처리가 끝난 데이터의 공분산은 $S=X^TX$이다.

$$
S=\begin{bmatrix}
s_{11}^2&s_{12}^2
\\
s_{21}^2&s_{22}^2
\end{bmatrix}=\frac{1}{m-1}\sum_{i=1}^m(x^{(i)}-\bar x)(x^{(i)}-\bar x)^T=X^TX
$$

이제 본격적으로 PCA를 이해할 준비를 마쳤다.

# 산포도

원 데이터 $X_0$을 산포도(Scatter Plot)로 표현해보자.

$X_0$의 2개의 열 $x_1,x_2$는 2가지 종류의 Feature를 나타내고(2개의 직교하는 좌표축),

$X_0$의 $m$개의 행 $x^{(i)T}$는 $m$개의 Sample Point $x^{(i)}$의 전치를 나타낸다(m개의 점).

전처리가 끝난 $X$도 비슷하다. 다만 Sample Point가 $z^{(i)}=(x^{(i)}-\bar x)/\sqrt{m-1}$로 바뀐다.

여기서 Centered(중심이 0) 와 Scaled(크기 조절)가 대략 무엇을 의미하는지 느낄 수 있다.

# 제1 주성분

그림에서 보면 Sample Point가 길쭉한 타원형으로 흩어져있다. 

따라서 $x_1,x_2$로 이 데이터를 설명하기 보다는 다음과 같이 새로운 좌표축, 즉 정규직교벡터 $v_1,v_2$를 찾아주는 것이 PCA의 목표이다.

$m$개의 데이터 점들을 어떤 벡터 $v$에 사영했을 때 전체적인 사영의 크기가 최대화된다면, 그 벡터 $v$는 PCA의 목표를 만족한다고 할 수 있다.

$i$번째 표본점 $z^{(i)}$의 $v$ 위로의 사영은

$$
Proj_v(z^{(i)})=\frac{v^Tz^{(i)}}{v^Tv}v=v^Tz^{(i)}v
$$

이된다. $v$를 단위벡터로 설정하자. 즉, $v^Tv=1$이다.

‘전체적인 사영의 크기를 최대화한다’는 것은 이 $m$개의 사영의 크기의 제곱합을 최대화하는 것.

다시 말해 아래와 같은 $v$의 함수 $P(v)$를 최대화하는 문제이다. ($1/(m-1)$는 상수니까 곱해도 됨.)

$$
P(v)=\frac{1}{m-1}\sum_{i=1}^m\Vert Proj_v(z^{(i)})\Vert^2
$$

식의 변형을 하자. 특히 사영의 크기의 제곱부분을 면밀히 살펴볼 필요가 있다. 

$v^Tz^{(i)}=z^{(i)T}v$는 스칼라, 절대값의 제곱은 그냥 제곱, $v$가 단위벡터라는 것을 기억하자.

$$
\Vert Proj_v(z^{(i)})\Vert^2=\Vert v^Tz^{(i)}v\Vert^2=\vert v^Tz^{(i)}\vert^2=(v^Tz^{(i)})^2=v^Tz^{(i)}z^{(i)T}v
$$

이므로

$$
P(v)=\frac{1}{m-1}\sum_{i=1}^mv^Tz^{(i)}z^{(i)T}v=v^T(\frac{1}{m-1}\sum_{i=1}^mz^{(i)}z^{(i)T})v=v^TSv
$$

에서 알 수 있듯이, 

전체적인 사영의 크기를 최대화하는 것은 $v^Tv=1$이라는 제약이 주어졌을 때 이차형식 $v^TSv$를 최대화하는 것과 같다.

이를 토대로 $Sv=\lambda v,\lambda=v^TSv$를 얻고, 전체적인 사영의 크기를 최대화하는 $v$는 공분산 행렬 $S$의 고유벡터이고, 고유치는 전체적인 사영의 크기 그 자체임을 알 수 있다.

> note: Rayleigh Quotient 및 제약조건 하에 이차형식의 최대, 최소를 구하는 지식이 필요하다.

데이터의 전체적인 사영의 크기 $P(v)=\frac{1}{m-1}\sum_{i=1}^m\Vert Proj_v(z^{(i)})\Vert^2$를 최대화하는 것은, 공분산 행렬 $S$의 고유치를 최대화 하는 것과 같다.

즉, 공분산 행렬의 최대 고유치가 최대 사영과 같다: $\max \lambda =\max P(v)$.

이때, **최대 사영을 만드는 $v$를 제 1 주성분이라고 하며, $v_1$로 나타낸다.**

이런 방식으로 $v_2$는 제 2 주성분이 된다.

제 3 주성분은 없다. $S$는 $2\times2$ 행렬이고 고유벡터는 2개 뿐이기 때문이다.

데이터의 열이 3개라면, 즉 Feature가 3종류라면 $S$가 $3\times 3$이 되고, 제 3 주성분이 가능하다.

# PCA와 관련된 의문

지금까지의 논의를 살펴보면 **PCA는 공분산행렬의 고유분해를 통해 이루어진다.**
<br>→ 공분산행렬의 최대 고유치에 대응하는 고유벡터가 제 1 주성분이다.
<br>→ 데이터를 제 1 주성분 위로 사영했을 때 사영의 크기가 최대화된다.
<br>→ 그 다음으로 사영의 크기가 큰 것은 제 2 주성분이고, 이런식으로 계속해서 주성분을 뽑아 낼 수  있다.

의문: 이제 다음 의문들을 해결하자. 주성분 분석에서 핵심적인 질문이다.
<br>(1) 주성분은 정규직교 벡터인가? (주성분이 좌표축의 역할을 하려면 그래야 함.)
<br>(2) 주성분의 개수는 몇 개인가?
<br>(3) 원래의 변수 대신 주성분으로 데이터를 설명할 때 데이터가 온전히 설명되는가?

## 주성분은 정규직교한다.

공분산 행렬 $S=X^TX$는 Square, Symmetric, Postive Semidefinite임을 알 수 있고,

Spectral Theorem에 의해 $S=V\Lambda V^T$로 분해된다.

$\Lambda$는 $S$의 고유치 $\lambda_i$들로 구성된 대각행렬이고,

$V$는 $S$의 정규직교 고유벡터(=주성분) $v_j$를 열로 갖는다.

## 주성분의 개수

주성분은 공분산 행렬 $S$의 고유벡터이므로 그 개수는 $S$의 사이즈에 따라 달렸다.

$S$가 2x2이면 주성분이 2개이고, 3x3이면 주성분이 3개이다.

한편, 공분산 행렬의 사이즈는 데이터의 열(Variable, Feature) 개수로 결정된다.

따라서 원래 3개의 좌표축으로 데이터를 설명했으면 주성분도 3개이다. 

이는 상식적으로도 합리적이다.

## 주성분 분석과 특이값 분해의 연관성

데이터 행렬 $X$를 특이값 분해하면 $X=U\Sigma V^T$인데,

여기서 $X$의 우특이벡터 $v_j$는 공분산 행렬 $X^TX=S$의 정규직교 고유벡터이고,

$X$의 특이값은 $\sigma_j=\sqrt{\lambda_j}$이다.

핵심은 **데이터 행렬 $X$의 특이값 분해에 주성분 분석에서 사용되는 공분산 행렬 $S=X^TX$가 그대로 사용된다는 것이다.**

> note: 따라서 따로 공분산 행렬을 계산하고 그것을 고유분해하는 과정을 거칠 필요 없이 곧 바로 데이터 행렬에 특이값 분해를 적용하면 된다. 물론 전처리는 필요하다.
> 

## 주성분의 설명력

$n$개의 독립된 Feature를 갖는 데이터의 주성분 $v_j$는 $n$개가 있다.

$X$의 Feature가 모두 독립이라는 가정하에 $r=n$, Full Column Rank가 된다. $r$은 $X$의 Rank이다.

데이터 행렬 $X$의 특이값 분해를 $r=n$개의 항으로 전개하자. 

$$
X=U\Sigma V^T=\sigma_1u_1v_1^T+\cdots+\sigma_nu_nv_n^T
$$

여기서 $\sigma_j$가 크기가 큰 순으로 정렬되어 있다는 사실이 중요하다.

가장 큰 $\sigma_1=\sqrt{\lambda_1}$은 제 1 주성분 $v_1$에 대응된다. 

제 1 주성분은 데이터 사영을 최대화하는 방향이고, 제 2 주성분은 그 다음으로 데이터 사영을 최대화하는 방향이다. 이런식으로 이어진다.

그러므로 $n$개의 항은 중요도 순으로 정렬되었으며, 뒤로 갈 수록 중요도가 떨어진다고 볼 수 있다.

만약 상대적으로 중요한 주성분 $v_1,v_2$만으로 $X$를 설명하고 나머지 항은 버린다면 그것은 $X$의 근사를 설명한 것이다. (SVD에서 Effective Rank = 2를 설정한 것과 같다.)

$$
X=U\Sigma V^T\approx\sigma_1u_1v_1^T+\sigma_2u_2v_2^T
$$

만약 $\sigma_3$부터 $0$이나 다름없는 매우 작은 값이라면, $0$을 버리는 것이므로 별 영향이 없다.

반대로 근사에 영향을 줄 정도로 큰 값이라면 데이터의 상당 부분 손실이 있게 된다.

**아무 것도 버리지 않는다면 온전하게($100\%$의) 데이터를 설명할 수 있다. 그러나 이는 PCA의 차원축소라는 목적과 어긋난다.**

추가 질문: 그렇다면 차원축소를 위해서 주성분을 버릴 때 손실되는 데이터의 비율은 구체적으로 어떻게 계산되는가?

행렬의 Trace(주대각합)는 고유치의 합과 같다는 사실을 이용해서,

**총변동성 = 공분산 행렬 고유치의 합 = 데이터 행렬 특이값의 제곱합**

을 유도할 수 있다. ($s_{jj}^2=s_j^2$이라고 쓰자.)

$$
Trace(S)=s_1^2+s_2^2=\lambda_1+\lambda_2=\sigma_1^2+\sigma_2^2
$$

여기서 $s_1^2+s_2^2$을 변수 $x_1,x_2$의 변동성(= 분산, Variance)을 모두 더했다고 해서 총변동성(Total Variance)이라고 한다.

총변동성에서 공분산 행렬의 고유치가 차지하는 비율 = 데이터 행렬의 특이값 제곱이 차지하는 비율

$\lambda_j/Trace(S)=\sigma_j^2/Trace(S)$는 

주성분 $v_j$가 데이터를 몇 퍼센트 설명할 수 있는지를 나타내는 지표 = **주성분의 데이터 설명력**이다.

주성분을 하나씩 버릴 때마다 설명할 수 있는 비율은 줄어들 것이다.
<br>→ **설명력이 큰 주성분을 버리면, 설명할 수 있는 비율은 왕창 줄어든다.**
<br>→ 설명력이 작은 주성분을 버리면, 설명할 수 있는 비율은 거의 변화가 없다.

이 사실을 그래프로 표현한 것이 Scree Plot이다. 

![alt text](/images/2024-09-29-01/ScreePlot.png)

그림에서 세로축은 제 1 주성분부터 순차적으로 제거할 때 데이터의 설명력이며 팍 꺾이는 곳부터 설명력의 감소가 미미하다.

따라서 **팍 꺾이는 곳 이전**까지는 설명력이 크다는 것을 의미하고, 거기까지의 주성분들은 **버리면 안된다!**

Scree Plot을 보고 주성분을 몇 개까지 허용할 것인가를 논할 수 있지만

아예 처음부터 목표 설명력(예: 적어도 80%)을 보고 그에 맞추어 개수를 택할 수도 있다.

## 주성분의 Loading

| Sample | 키($x_1$) | 몸무게($x_2$) | 제 1 주성분($v_1$) | 제 2 주성분($v_2$) |
| --- | --- | --- | --- | --- |
| $1$ | Sample $1$의 키 | Sample $1$의 몸무게 | Sample $1$의 제 1 주성분Loading | Sample $1$의 제 2 주성분 Loading |
| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| $m$ | Sample $m$의 키 | Sample $m$의 몸무게 | Sample $m$의 제 1 주성분Loading | Sample $m$의 제 2 주성분 Loading |

주성분의 값을 Loading이라고 한다.

어떤 Sample Point $z^{(i)}$의 주성분 $v$ Loading은 그 Sample Point의 주성분 $v$ 위로의 사영이다:

$$
Proj_v(z^{(i)})=z^{(i)T}v/v^Tv=z^{(i)T}v
$$

따라서 $XV$로 모든 Loading을 나타낸다. 

$XV$의 $j$열은 모든 Sample의 제 $j$ 주성분 Loading을,

$XV$의 $i$행은 Sample $i$의 모든 Loading을 나타낸다.

$$
Loadings=XV=
\begin{bmatrix}
-z^{(1)T}-
\\
\vdots
\\
-z^{(m)T}-
\end{bmatrix}
\begin{bmatrix}
v_1&v_2
\end{bmatrix}
=
\begin{bmatrix}
z^{(1)T}v_1&z^{(1)T}v_2
\\
\vdots&\vdots
\\
z^{(m)T}v_1&z^{(m)T}v_2
\end{bmatrix}
$$