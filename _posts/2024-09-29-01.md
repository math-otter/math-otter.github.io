---
title: "PCA (6): 주성분"
categories: 수학
tags: [선형대수학, 수리통계학, 데이터 분석]
---

PCA(주성분분석, Principal Component Analysis)를 선형대수의 지식으로 이해하자.

지금까지의 논의를 살펴보면 **PCA는 공분산행렬의 고유분해를 통해 이루어진다.**
<br>→ 공분산행렬의 최대 고유치에 대응하는 고유벡터가 제 1 주성분이다.
<br>→ 데이터를 제 1 주성분 위로 사영했을 때 사영의 크기가 최대화된다.
<br>→ 그 다음으로 사영의 크기가 큰 것은 제 2 주성분이고, 이런식으로 계속해서 주성분을 뽑아 낼 수  있다.

의문: 이제 다음 의문들을 해결하자. 주성분 분석에서 핵심적인 질문이다.
<br>(1) 주성분은 정규직교 벡터인가? (주성분이 좌표축의 역할을 하려면 그래야 함.)
<br>(2) 주성분의 개수는 몇 개인가?
<br>(3) 원래의 변수 대신 주성분으로 데이터를 설명할 때 데이터가 온전히 설명되는가?

# 주성분은 정규직교한다.

공분산 행렬 $S=X^TX$는 Square, Symmetric, Postive Semidefinite임을 알 수 있고,

Spectral Theorem에 의해 $S=V\Lambda V^T$로 분해된다.

$\Lambda$는 $S$의 고유치 $\lambda_i$들로 구성된 대각행렬이고,

$V$는 $S$의 정규직교 고유벡터(=주성분) $v_j$를 열로 갖는다.

# 주성분의 개수

주성분은 공분산 행렬 $S$의 고유벡터이므로 그 개수는 $S$의 사이즈에 따라 달렸다.

$S$가 2x2이면 주성분이 2개이고, 3x3이면 주성분이 3개이다.

한편, 공분산 행렬의 사이즈는 데이터의 열(Variable, Feature) 개수로 결정된다.

따라서 원래 3개의 좌표축으로 데이터를 설명했으면 주성분도 3개이다. 

이는 상식적으로도 합리적이다.

# 주성분 분석과 특이값 분해의 연관성

데이터 행렬 $X$를 특이값 분해하면 $X=U\Sigma V^T$인데,

여기서 $X$의 우특이벡터 $v_j$는 공분산 행렬 $X^TX=S$의 정규직교 고유벡터이고,

$X$의 특이값은 $\sigma_j=\sqrt{\lambda_j}$이다.

핵심은 **데이터 행렬 $X$의 특이값 분해에 주성분 분석에서 사용되는 공분산 행렬 $S=X^TX$가 그대로 사용된다는 것이다.**

> note: 따라서 따로 공분산 행렬을 계산하고 그것을 고유분해하는 과정을 거칠 필요 없이 곧 바로 데이터 행렬에 특이값 분해를 적용하면 된다. 물론 전처리는 필요하다.
> 

# 주성분의 설명력

$n$개의 독립된 Feature를 갖는 데이터의 주성분 $v_j$는 $n$개가 있다.

$X$의 Feature가 모두 독립이라는 가정하에 $r=n$, Full Column Rank가 된다. $r$은 $X$의 Rank이다.

데이터 행렬 $X$의 특이값 분해를 $r=n$개의 항으로 전개하자. 

$$
X=U\Sigma V^T=\sigma_1u_1v_1^T+\cdots+\sigma_nu_nv_n^T
$$

여기서 $\sigma_j$가 크기가 큰 순으로 정렬되어 있다는 사실이 중요하다.

가장 큰 $\sigma_1=\sqrt{\lambda_1}$은 제 1 주성분 $v_1$에 대응된다. 

제 1 주성분은 데이터 사영을 최대화하는 방향이고, 제 2 주성분은 그 다음으로 데이터 사영을 최대화하는 방향이다. 이런식으로 이어진다.

그러므로 $n$개의 항은 중요도 순으로 정렬되었으며, 뒤로 갈 수록 중요도가 떨어진다고 볼 수 있다.

만약 상대적으로 중요한 주성분 $v_1,v_2$만으로 $X$를 설명하고 나머지 항은 버린다면 그것은 $X$의 근사를 설명한 것이다. (SVD에서 Effective Rank = 2를 설정한 것과 같다.)

$$
X=U\Sigma V^T\approx\sigma_1u_1v_1^T+\sigma_2u_2v_2^T
$$

만약 $\sigma_3$부터 $0$이나 다름없는 매우 작은 값이라면, $0$을 버리는 것이므로 별 영향이 없다.

반대로 근사에 영향을 줄 정도로 큰 값이라면 데이터의 상당 부분 손실이 있게 된다.

**아무 것도 버리지 않는다면 온전하게($100\%$의) 데이터를 설명할 수 있다. 그러나 이는 PCA의 차원축소라는 목적과 어긋난다.**

추가 질문: 그렇다면 차원축소를 위해서 주성분을 버릴 때 손실되는 데이터의 비율은 구체적으로 어떻게 계산되는가?

행렬의 Trace(주대각합)는 고유치의 합과 같다는 사실을 이용해서,

**총변동성 = 공분산 행렬 고유치의 합 = 데이터 행렬 특이값의 제곱합**

을 유도할 수 있다. ($s_{jj}^2=s_j^2$이라고 쓰자.)

$$
Trace(S)=s_1^2+s_2^2=\lambda_1+\lambda_2=\sigma_1^2+\sigma_2^2
$$

여기서 $s_1^2+s_2^2$을 변수 $x_1,x_2$의 변동성(= 분산, Variance)을 모두 더했다고 해서 총변동성(Total Variance)이라고 한다.

총변동성에서 공분산 행렬의 고유치가 차지하는 비율 = 데이터 행렬의 특이값 제곱이 차지하는 비율

$\lambda_j/Trace(S)=\sigma_j^2/Trace(S)$는 

주성분 $v_j$가 데이터를 몇 퍼센트 설명할 수 있는지를 나타내는 지표 = **주성분의 데이터 설명력**이다.

주성분을 하나씩 버릴 때마다 설명할 수 있는 비율은 줄어들 것이다.
<br>→ **설명력이 큰 주성분을 버리면, 설명할 수 있는 비율은 왕창 줄어든다.**
<br>→ 설명력이 작은 주성분을 버리면, 설명할 수 있는 비율은 거의 변화가 없다.

이 사실을 그래프로 표현한 것이 Scree Plot이다. 

![alt text](/images/2024-09-29-01/ScreePlot.png)

그림에서 세로축은 제 1 주성분부터 순차적으로 제거할 때 데이터의 설명력이며 팍 꺾이는 곳부터 설명력의 감소가 미미하다.

따라서 **팍 꺾이는 곳 이전**까지는 설명력이 크다는 것을 의미하고, 거기까지의 주성분들은 **버리면 안된다!**

Scree Plot을 보고 주성분을 몇 개까지 허용할 것인가를 논할 수 있지만

아예 처음부터 목표 설명력(예: 적어도 80%)을 보고 그에 맞추어 개수를 택할 수도 있다.

# 주성분의 Loading

| Sample | 키($x_1$) | 몸무게($x_2$) | 제 1 주성분($v_1$) | 제 2 주성분($v_2$) |
| --- | --- | --- | --- | --- |
| $1$ | Sample $1$의 키 | Sample $1$의 몸무게 | Sample $1$의 
제 1 주성분Loading | Sample $1$의
제 2 주성분 Loading |
| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| $m$ | Sample $m$의 키 | Sample $m$의 몸무게 | Sample $m$의 
제 1 주성분Loading | Sample $m$의
제 2 주성분 Loading |

주성분의 값을 Loading이라고 한다.

어떤 Sample Point $z^{(i)}$의 주성분 $v$ Loading은 그 Sample Point의 주성분 $v$ 위로의 사영이다:

$$
Proj_v(z^{(i)})=z^{(i)T}v/v^Tv=z^{(i)T}v
$$

따라서 $XV$로 모든 Loading을 나타낸다. 

$XV$의 $j$열은 모든 Sample의 제 $j$ 주성분 Loading을,

$XV$의 $i$행은 Sample $i$의 모든 Loading을 나타낸다.

$$
Loadings=XV=
\begin{bmatrix}
-z^{(1)T}-
\\
\vdots
\\
-z^{(m)T}-
\end{bmatrix}
\begin{bmatrix}
v_1&v_2
\end{bmatrix}
=
\begin{bmatrix}
z^{(1)T}v_1&z^{(1)T}v_2
\\
\vdots&\vdots
\\
z^{(m)T}v_1&z^{(m)T}v_2
\end{bmatrix}
$$