---
title: "Chapter 06: 비지도 학습 - 2. k-평균"
categories: 코딩
tags: [파이썬, 데이터 분석]
---

# k-평균 군집 알고리즘의 아이디어

[Chapter 06: 비지도 학습 - 1. 군집 알고리즘]
에서는 300장의 사진이 순서(사과, 파인애플, 바나나)대로 정렬이 되어 있어, 각 클러스터의 기준이 되는 평균을 계산할 수 있었고, 이미지로 출력도 가능했다.

실전에서는 수많은 사진이 순서대로 정렬되어 있지 않을 뿐더러, 어떤 사진인지도 분류를 하기 전까지는 알 수 없다.

이럴 때 머신러닝 모델이 알고리즘에 의해 스스로 기준을 찾도록 만들어야 한다.

![alt text](/images/2024-09-28-01/01-kmeans.gif)

관련 알고리즘 중 대표적인 것이 **k-평균 군집 알고리즘**이다. 

이 알고리즘은 다음과 같은 과정을 통해 클러스터 중심(Cluster Center) 또는 센트로이드(Centroid)라고 부르는 기준을 찾는다.

1. 클러스터 개수 k를 정한다.
2. k개의 중심(센트로이드)을 랜덤으로 할당한다. k개의 중심은 k개의 클러스터를 의미한다.
3. 각 샘플을 가까운(거리가 작은) 중심에 할당한다.
4. 할당된 샘플을 토대로 중심을 다시 계산한다.
5. 위 2~3 과정을 반복하여 Inertia를 최소로 한다.


**Inertia(이너셔): Within-Cluster Sum of Squares (WCSS)라고도 하는, 군집 알고리즘의 평가지표이다.**

먼저 $j$번째 클러스터 $C_j$에 속한 샘플 $x$들의 그 클러스터의 중심 $\mu_j$에 이르는 전체적인 유클리드 거리는 다음과 같이 계산한다.

$$
I_j=\sum_{x\in C_j}\Vert x-\mu_j \Vert^2
$$

k-means 알고리즘에서 클러스터는 $k$개가 있다. 이너셔는 $j=1,\cdots,k$에 대해, 각 클러스터의 $I_j$를 합산한 것이다.

$$
I=\sum_{j=1}^k\sum_{x\in C_j}\Vert x-\mu_j \Vert^2
$$

유클리드 거리는 스케일링에 취약하다는 단점이 있다. 이것이 문제될 때는 클러스터의 공분산을 고려한 마할라노비스 거리를 대입하면 된다.

(마할라노비스 거리는 유클리드 거리의 일반화로 볼 수 있다. 공분산 행렬 $\Sigma_j$가 항등행렬일 때, 마할라노비스 거리와 유클리드 거리는 같게 된다.)

$$
I=\sum_{j=1}^k\sum_{x\in C_j}(x-\mu_j)^T\Sigma_j^{-1}(x-\mu_j)
$$

# k-평균 알고리즘 용례

예를 들어 다음과 같은 2개의 특징 x, y를 가진 데이터 300개가 있다고 하자.

|  | x | y |
| --- | --- | --- |
| 0 |  |  |
| 1 |  |  |
| … | … | … |
| 299 |  |  |

k-평균 알고리즘을 통해, 이 50개의 데이터를 3개로 분류한다면, k=3으로 설정하면 될 것이다. 그러면 결과적으로 3개의 클러스터 0, 1, 2를 얻으며, 각 샘플이 속한 클러스터를 다음과 같이 표시할 수 있을 것이다.

|  | x | y | 클러스터 |
| --- | --- | --- | --- |
| 0 |  |  | 0 |
| 1 |  |  | 1 |
| … | … | … | … |
| 299 |  |  | 2 |

지금 분류하려는 과일 이미지도 300개가 있고, k=3인 경우이다. 다만, 특징의 개수는 100*100=1만개이다. 1만개의 픽셀 값들이 1만개의 특징이 되는 것이다.

|  | 픽셀0 | 픽셀1 | … | 픽셀9999 | 클러스터 |
| --- | --- | --- | --- | --- | --- |
| 0 |  |  | … |  | 0 |
| 1 |  |  | … |  | 1 |
| … | … | … | … | … | … |
| 299 |  |  | … |  | 2 |

여기서 0이 어떤 종류의 과일을 나타내는지는 분류를 수행하고 이미지를 출력했을 때 비로소 알 수 있다. 

→ 컴퓨터는 k-평균 알고리즘을 통해 주어진 데이터를 분류할 뿐, 각 클러스터에 의미를 부여하지 않는다. 클러스터에 의미를 부여하는 것은 인간이다.

# 사이킷런의 k-평균 알고리즘 사용

다른 사이킷런의 모델과 사용법이 비슷하다.

→ k를 설정하는 것은 `n_clusters`이다.

→ k-평균 알고리즘도 랜덤요소가 있으므로(시작시 센트로이드) `random_state`를 지정해야 코드를 실행할 때마다 일관된 결과를 얻을 수 있다.

→ 다른 모델과 마찬가지로, 훈련 데이터는 2차원 배열로 전달되어야 한다. `reshape()` 메서드로 배열의 차원을 변형한다.

→ 정답이 없는 비지도 학습이므로, `fit()` 메서드에 훈련데이터만 전달한다.

→ 분류 결과는 `labels_` 속성에 저장된다.

→ 알고리즘이 반복한 횟수는 `n_iter_` 속성에 저장된다.

```python
!wget https://bit.ly/fruits_300_data -O fruits_300.npy
import numpy as np
fruits = np.load('fruits_300.npy')

from sklearn.cluster import KMeans
km = KMeans(n_clusters=3, random_state=42) # 모델 객체 생성
km.fit(fruits.reshape(-1, 100*100)) # 모델 객체 훈련

result = km.labels_
n_iter = km.n_iter_
print(result)
print(n_iter)

결과:
[2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 ... 1]
4
```

# 클러스터 중심 출력과 의미 부여

컴퓨터는 k-평균 알고리즘에 따라 착실하게 이너셔를 줄여가며 최적의 클러스터 중심을 찾았을 뿐이다. 레이블에 의미를 부여하는 것은 사용자인 인간이다. 

각 클러스터 중심(센트로이드)은 `cluster_centers` 속성에 저장된다. 앞서 정의한 `draw_images()`함수를 이용하여 k-평균 알고리즘으로 구한 평균과 각 과일의 실제 평균을 그려보고 비교해본다.

```python
centroids = km.cluster_centers_.reshape(3, 100, 100)
means = np.array([np.mean(_, axis=0) for _ in [fruits[0:100], fruits[100:200], fruits[200:300]]])

draw_images(centroids, ratio=3, titles=['0', '1', '2'], axis='on')
draw_images(means, ratio=3, titles=['apple', 'pineapple', 'banana'], axis='on')
```
![alt text](/images/2024-09-28-01/02a-kmeans과일평균.png)
![alt text](/images/2024-09-28-01/02b-실제과일평균.png)

레이블 0, 1, 2의 올바른 의미는 파인애플, 바나나, 사과이다.

만약 이미지 데이터가 순서대로 정렬되어 있지 않아 각 이미지 카테고리의 실제 평균을 찾을 수 없다면(대부분의 경우가 이에 속함), 주로 어떤 이미지가 동일한 레이블에 분류되었는지 본다. 예를 들어 레이블 1의 의미가 궁금하다면, 다음과 같이 레이블이 1로 분류된 것만 시각화해본다.

```python
draw_images(fruits[km.labels_==1], ncols=20)
```

![alt text](/images/2024-09-28-01/03a-bananas.png)

바나나 이미지 98개가 출력되었다. 따라서 레이블 1은 바나나를 의미한다.

레이블 2로 분류한 이미지는 다음과 같다. 전부 사과 이미지이다. 따라서 레이블 2는 사과를 의미한다.

![alt text](/images/2024-09-28-01/03b-apples.png)

레이블 0으로 분류한 이미지는 다음과 같다. 사과와 바나나가 섞여있지만, 대부분이 파인애플이다. 따라서 레이블 0을 파인애플이라고 정하자.

![alt text](/images/2024-09-28-01/03c-pineapples.png)

# 거리 출력

`transform()` 메서드는 훈련 데이터 샘플에서 각 클러스터 중심까지 거리로 변환해주는 기능이 있다.

→ 인덱스가 42인 샘플의 각 클러스터 중심까지의 거리를 계산해보자. `fruits[42]`처럼 쓰면 (10000,) 크기의 1차원 배열이 되므로 에러가 발생한다. 따라서 `fruits[42:43]` 같이 입력하여 2d 배열을 보장한다.

→ `fit()` 메서드와 마찬가지로, 2d 배열로 변형하여 전달한다. 

→ 샘플이 1개, 클러스터가 3개이므로, 결과물은 (1, 3) 크기의 2차원 배열이다.

```python
print(km.transform(fruits[42:43].reshape(-1, 100*100)))

결과:
[[4789.51 8684.2  5624.28]]
```

결과물의 행은 특정 샘플을, 열은 해당 샘플과 각 클러스터(0, 1, 2) 까지의 거리를 나타낸다. 지금 클러스터 0까지의 거리가 가장 작으므로 해당 샘플은 레이블 0으로 분류될 것이다.

# 예측 클래스 출력

`predict()` 메서드는 예측 클래스(가장 가까운 클러스터 중심)를 출력한다.

→ `labels_` 속성도 클래스를 출력하지만, 이는 학습 과정에서 분류한 클래스를 출력하는 것이고, `predict()` 메서드는 학습한 결과를 토대로 학습 데이터뿐 아니라 다른 데이터의 클래스도 예측할 수 있다. (학습 데이터를 전달하면 `labels_` 속성으로 얻은 결과와 같을 것이다.)

```python
print(km.predict(fruits[42:43].reshape(-1, 100*100)))
print(km.labels_[42:43])
draw_images(fruits[42:43])

결과:
[0]
[0]
```

![alt text](/images/2024-09-28-01/04-눕혀놓은사과.png)

앞에서 레이블 0의 의미를 파인애플이라고 정했지만, 실제 이 샘플을 그려보면, 눕혀놓은 사과 이미지를 얻는다. 그러나 이것을 틀리게 분류했다고 할 수 없다. 비지도 학습에서 정답이란 없기 때문이다. 

분류 결과를 보고 레이블에 의미를 부여하는 것은 인간이며, 컴퓨터는 그저 주어진 알고리즘에 따라 레이블을 정한 것이다.

# 엘보우 방법: 이너셔의 감소 추세를 보고 적절한 k 찾기

k-평균 알고리즘은 클러스터 개수를 사전에 지정해야한다는 단점이 있다. 지금까지는 3종류의 과일(사과, 파인애플, 바나나)을 분류한다는 명확한 목표가 있기 때문에, 클러스터 개수를 3이라고 할 수 있었다.

그러나 실전에서는 분류에 정답이 없는 데이터를 분류하는데, 당연히 몇 개의 클러스터가 존재할지도 알 수 없다. 하지만 적절한 k를 찾는 방법이 존재하는데, 바로 이너셔를 대폭 줄이는 것이다.

여러 k값을 설정하여 이너셔를 저장한 후 그래프를 그려보자. 이너셔는 `inertia_` 속성에 저장된다.

```python
from sklearn.cluster import KMeans
inertia = []
k_range = range(2,7)
for k in k_range:
    km = KMeans(n_clusters=k, random_state=42) # 모델 객체 생성
    km.fit(fruits.reshape(-1, 100*100)) # 모델 객체 훈련
    inertia.append(km.inertia_)
plt.plot(k_range, inertia)
plt.xlabel('k')
plt.ylabel('inertia')
plt.show()
```

![alt text](/images/2024-09-28-01/05-엘보우방법.png)

이너셔 그래프는 k가 증가할 때 계속 감소한다. 이는 자연스러운 현상으로, 일반적으로 클러스터 개수가 늘어나면 클러스터 개개의 크기가 줄어들기 때문에 이너셔도 감소한다.

문제는 감소의 추세가 k=3 지점에서 변한다는 것이다. k=3까지는 이너셔가 상대적으로 가파르게 감소하는 반면, 그 이후로는 감소 추세가 완만하다. 따라서 k=3 지점을 적절한 k 값으로 정하는 것이다.

그래프가 마치 사람이 팔을 굽힌 모양을 하고 있고, 적절한 k는 이처럼 팔꿈치(그래프가 꺾이는 지점, 즉 감소추세가 변하는 지점)에 있으므로 이 방법을 엘보우(팔꿈치) 방법이라고 한다.