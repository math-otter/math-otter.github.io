---
title: "확률변수를 회귀함수와 잔차의 합으로 분해하기"
slug: "확률변수를 회귀함수와 잔차의 합으로 분해하기"
categories: 수학
tags: [수리통계학, 데이터 분석]
---

기호 $\oplus$을 이용하여 상관계수가 0인 확률변수의 합을 나타내자.

확률변수 $Y$는 다음과 같이 회귀함수 $E[Y\vert X]$와 잔차 $Y-E[Y\vert X]$의 합으로 분해된다.

$$
Y = E[Y\vert X] \oplus (Y-E[Y\vert X])
$$

$Y$의 회귀함수 $E[Y\vert X]=\hat Y$라고 하면

$$
Y = \hat Y \oplus (Y-\hat Y)
$$

# 증명

확률변수로서의 조건부기댓값의 성질 (4)에서 $(Y-\hat Y)$와 임의의 $X$ 관련 함수 $g(X)$의 상관계수가 0임을 알 수 있다. 

그런데 $Y$의 회귀함수 $\hat Y=E[Y\vert X]$는 $X$에 관한 함수이다. 따라서 $Cov[Y-\hat Y, \hat Y]=0$이다.

## 조건부기댓값의 성질

(1) 조건부기댓값의 선형성
$E[c_1g_1(Y)+c_2g_2(Y)\vert X=x]=c_1E[g_1(Y)\vert X=x]+c_2E[g_2(Y)\vert X=x]$

(2) Factor out the constant

$E[c(X)g(Y)\vert X=x]=c(x)E[g(Y)\vert X=x]$

## 확률변수로서의 조건부기댓값의 성질

(3) 전체 기댓값의 법칙: $E[E[Y\vert X]]=E[Y]$ (또는 $E[\hat Y]=E[Y]$)

- 증명
    
    이산형의 경우
    
    $$
    \begin{split}
    E[E[Y\vert X]]
    &=
    \sum_x E[Y\vert X=x]P(X=x)
    \\
    &=
    \sum_x (\sum_y yP(Y=y\vert X=x)) P(X=x)
    \\
    &=
    \sum_x \sum_y y(P(Y=y\vert X=x) P(X=x))
    \\
    &=
    \sum_x \sum_y yP(X=x,Y=y)
    \\
    &=
    \sum_y \sum_x yP(X=x,Y=y)
    \\
    &=
    \sum_y y\sum_x P(X=x,Y=y)
    \\
    &=
    \sum_y yP(Y=y)=E[Y]
    \end{split}
    $$
    
    연속형의 경우
    
    $$
    \begin{split}
    E[E[Y\vert X]]
    &=
    \int_{-\infty}^{+\infty}E[Y\vert X=x]f_X(x)dx
    \\
    &=
    \int_{-\infty}^{+\infty}(\int_{-\infty}^{+\infty}yf_{Y\vert X}(y\vert x)dy)f_X(x)dx
    \\
    &=
    \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}y(f_{Y\vert X}(y\vert x)f_X(x))dydx
    \\
    &=
    \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}yf(x,y)dydx
    \\
    &=
    \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}yf(x,y)dxdy
    \\
    &=
    \int_{-\infty}^{+\infty}y\int_{-\infty}^{+\infty}f(x,y)dxdy
    \\
    &=
    \int_{-\infty}^{+\infty}yf_Y(y)dy=E[Y]
    \end{split}
    $$
    

(4) $Cov[Y-E[Y\vert X], g(X)]=0, \forall g(X)$ (또는 $Cov[Y-\hat Y, g(X)]=0$)

- 증명
    
    $Z=Y-E[Y\vert X]$라고 하면,
    
    $$
    \begin{split}
    &E[Z\vert X]\overset{(1)}{=}E[Y\vert X]-E[E[Y\vert X]\vert X]\overset{(3)}{=}E[Y\vert X]-E[Y\vert X]=0
    \\
    &E[Z]\overset{(3)}{=}E[E[Z\vert X]]=E[0]=0
    \end{split}
    $$
    
    따라서 임의의 $g(X)$에 대해
    
    $$
    \begin{split}
    Cov[Z,g(X)]&=E[Zg(X)]-E[Z]E[g(X)]
    \\
    &=E[Zg(X)]-0\cdot E[g(X)]
    \\
    &=E[Zg(X)]
    \\
    &=E[E[Zg(X)\vert X]]
    \\
    &\overset{(2)}{=}E[g(X)E[Z\vert X]]=E[g(X)\cdot0]=0
    \end{split}
    $$
    

# 확장: 편차의 분해

$Y$의 평균 $E[Y]=\mu$라고 하면, 편차 $Y-\mu$를 다음과 같이 분해할 수 있다.

$$
Y -\mu= (\hat Y-\mu) \oplus (Y-\hat Y)
$$

이를 그림으로 표현해보자. 그림에서 직각은 상관계수가 0임을 뜻한다.

<aside>
💡 그림

</aside>

# 최소제곱예측자

확률변수 $Y$를 다른 확률변수 $X$의 함수 $g=g(X)$로 예측하려고 한다. 

이때, 예측의 오차는 $Y-g$이고 평균제곱예측오차는 $E[(Y-g)^2]$이다. 

모든 $X$ 관련 함수 중에서, 이 평균제곱예측오차를 최소화하는 함수는 $\hat Y=E[Y\vert X]$, 즉 $Y$의 회귀함수이다. 그리고 평균제곱예측오차의 최소값은 $E[(Y-\hat Y)^2]=E[Var[Y\vert X]]$이다

$$
E[(Y-g)^2]\ge E[(Y-\hat Y)^2]=E[Var[Y\vert X]],\forall g
$$

- 증명
    
    $Y-g$는 $\hat Y -g$라는 $X$관련 함수와 잔차 $Y-\hat Y$로 분해된다. 즉,
    
    $$
    \begin{split}
    (Y-g)^2&=[(\hat Y-g)\oplus(Y-\hat Y)]^2
    \\
    &=(\hat Y-g)^2+(Y-\hat Y)^2+2(\hat Y-g)(Y-\hat Y)
    \end{split}
    $$
    
    여기서 $E[(Y-g)^2]$을 계산하기 위해 각 항의 기댓값을 계산하려고한다. 그런데
    
    $$
    \begin{split}
    &E[(\hat Y-g)(Y-\hat Y)]=Cov[\hat Y-g,Y-\hat Y]+E[\hat Y-g]E[Y-\hat Y]=0,
    \\
    \\
    &Cov[\hat Y-g,Y-\hat Y]=0
    \\
    &E[Y-\hat Y]=E[Y]-E[\hat Y]=E[Y]-E[Y]=0
    \end{split}
    $$
    
    이므로 $E[(Y-g)^2]=E[(\hat Y-g)^2]+E[(Y-\hat Y)^2]\ge E[(Y-\hat Y)^2]$인 것이다. 등호는 $g=\hat Y$일 때, 즉 $X$의 함수가 $Y$의 회귀함수일 때 성립한다.
    
    그리고 $E[(Y-\hat Y)^2]=E[E[(Y-\hat Y)^2\vert X]]=E[Var[Y\vert X]]$이다.
    

# 분산의 분해

확률변수 $Y$의 분산은 다음과 같이 분해된다.

$$
Var[Y]=Var[E[Y\vert X]]+E[Var[Y\vert X]]
$$

$Y$와 회귀함수 $\hat Y$만 이용하여 간략히 하면

$$
Var[Y]=Var[\hat Y]+E[(Y-\hat Y)^2]
$$

기댓값만을 이용하여 간략히 하면

$$
E[(Y-\mu)^2]=E[(\hat Y-\mu)^2]+E[(Y-\hat Y)^2]
$$

- 증명
    
    편차의 분해 $Y-\mu=(\hat Y-\mu)\oplus(Y-\hat Y)$에서 출발한다. 양변을 제곱하고 기댓값을 취하면
    
    $$
    E[(Y-\mu)^2]=E[(\hat Y-\mu)^2]+E[(Y-\hat Y)^2]+2E[(\hat Y-\mu)(Y-\hat Y)]
    $$
    
    가 되는데, 여기서 
    
    $$
    \begin{split}
    &E[(\hat Y-\mu)(Y-\hat Y)]=Cov[\hat Y-\mu,Y-\hat Y]+E[\hat Y-\mu]E[Y-\hat Y]=0,
    \\
    \\
    &Cov[\hat Y-\mu,Y-\hat Y]=0
    \\
    &E[Y-\hat Y]=E[Y]-E[\hat Y]=E[Y]-E[Y]=0
    \end{split}
    $$
    
    이므로 $E[(Y-\mu)^2]=E[(\hat Y-\mu)^2]+E[(Y-\hat Y)^2]$이다. 그리고 다음과 같이 대응된다.
    
    $E[(Y-\mu)^2]=Var[Y]$ (분산의 정의)
    
    $E[(\hat Y-\mu)^2]=Var[\hat Y]$ (분산의 정의와 $\mu=E[Y]=E[\hat Y]$)
    
    $E[(Y-\hat Y)^2]=E[Var[Y\vert X]]$ (전체 기댓값의 법칙, 조건부분산의 정의)